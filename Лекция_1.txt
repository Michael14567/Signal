ОБРАБОТКА И ИНТЕРПРЕТАЦИЯ СИГНАЛОВ Лекция 1. 

                       
  1. Общая идея курса
                       
Несмотря на «цифровую эпоху», физическая природа сигналов остаётся
аналоговой. Любая передача подвержена шуму → возможны искажения.

Главная задача: обеспечить надёжную передачу информации при наличии
шума.

                                               
  2. Упрощённая модель цифровой системы связи
                                               

Источник данных → Кодер → Канал с шумом → Декодер → Получатель.

Источник генерирует информационные символы (обычно биты 0 и 1).

Кодер:   преобразует информационные символы,   добавляет избыточность.

Канал:   вносит ошибки с вероятностью p.

Декодер:   пытается восстановить исходные символы,   используя
избыточность.

                                        
  3. Двоичный симметричный канал (ДСК)
                                        

На вход подаётся 0 или 1.

С вероятностью p: 0 → 1 1 → 0

С вероятностью 1 − p: символ не изменяется.

p — переходная вероятность (вероятность ошибки одного символа).

                       
  4. Идея кодирования
                       

Если передавать биты напрямую: вероятность ошибки = p.

Идея: дублировать данные.

Пример: 0 → 000 1 → 111

Декодирование — по правилу большинства.

Вероятность ошибки уменьшается, но:   уменьшается скорость передачи,  
увеличивается длина сообщений.

Плата за надёжность — избыточность.

                    
  5. Скорость кода
                    

Пусть: k — число информационных символов, n — длина кодового слова.

Скорость: R = k / n

Без кодирования: R = 1 При повторении трёх раз: R = 1/3 В более сложном
коде (2,5): R = 2/5

Чем меньше R, тем больше избыточность.

                      
  6. Теорема Шеннона
                      

Для двоичного симметричного канала вводится пропускная способность:

C = 1 − H(p)

где H(p) = −p log2 p − (1−p) log2 (1−p)

Если R < C, можно добиться сколь угодно малой вероятности ошибки,
увеличивая длину кода.

Если R > C, надёжная передача невозможна.

Теорема неконструктивная: существование доказано, конкретная конструкция
не указана.

                          
  7. Расстояние Хэмминга
                          

Вес Хэмминга: число ненулевых элементов в слове.

Расстояние Хэмминга: число позиций, в которых два слова различаются.

Для двоичных кодов: d(x,y) = вес(x + y)

Минимальное расстояние кода: d_min = минимум расстояний между различными
кодовыми словами.

Код исправляет t ошибок, если: t ≤ ⌊(d_min − 1)/2⌋

                    
  8. Линейные коды
                    

Линейный код: сумма любых двух кодовых слов — кодовое слово.

Код является подпространством пространства GF(q)^n.

Обозначение: (n, k) код   n — длина кодового слова,   k — число
информационных символов.

Скорость: R = k/n

                          
  9. Порождающая матрица
                          

Код задаётся матрицей G размера k × n.

Строки G — базисные векторы.

Кодовое слово: c = uG

где u — информационный вектор.

                           
  10. Проверочная матрица
                           

Существует матрица H размера (n−k) × n,

такая что: c H^T = 0 для любого кодового слова c.

H называется проверочной матрицей.

Она позволяет:   проверять принадлежность слова коду,   обнаруживать
ошибки.

                           
  11. Главные идеи лекции
                           

1.  Шум неизбежен.
2.  Избыточность позволяет исправлять ошибки.
3.  Надёжность и скорость противоречат друг другу.
4.  Минимальное расстояние определяет исправляющую способность.
5.  Линейные коды удобны благодаря матричному описанию.
6.  Теорема Шеннона задаёт фундаментальный предел передачи.
